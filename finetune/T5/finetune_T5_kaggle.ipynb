{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864ab9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HocTap\\ChatBot\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Seed set to 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration  \n",
    "\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "pl.seed_everything(100)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116ffdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1.post0\n"
     ]
    }
   ],
   "source": [
    "# mostly pl is used while doing complex model training\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf3a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparse makes it easier to write user friendly command line interfaces\n",
    "import argparse\n",
    "# package for faster file name matching\n",
    "import glob\n",
    "# makiing directories for data \n",
    "import os\n",
    "# reading json files as the data is present in json files\n",
    "import json\n",
    "# time module for calculating the model runtime\n",
    "import time\n",
    "# Allows writing status messages to a file\n",
    "import logging\n",
    "# generate random float numbers uniformly\n",
    "import random\n",
    "# regex module for text \n",
    "import re\n",
    "# module provides various functions which work on \n",
    "# iterators too produce complex iterators\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "# pandas for data manipulation\n",
    "import pandas as pd\n",
    "# numpy for array operations\n",
    "import numpy as np\n",
    "# PyTorch\n",
    "import torch\n",
    "# provides various classes representing file system paths\n",
    "# with appropriate semantics\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# splitting the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "# ANSII color formatting for ouput in terminal\n",
    "from termcolor import colored\n",
    "# wrapping paragraphs into string\n",
    "import textwrap\n",
    "\n",
    "# model checkpoints in pretrained model\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "'''\n",
    "optimizer - AdamW\n",
    "T5 Conditional Generator in which we'll give conditions\n",
    "T5 tokenizer because it is fast\n",
    "training the model without a learning rate\n",
    "'''\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6a61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1.post0\n"
     ]
    }
   ],
   "source": [
    "# check the version provided by Lightning\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670d036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Question', 'Context', 'Answer', 'Answer_Start', 'Answer_End'],\n",
      "        num_rows: 197379\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files='../dataset/QA_data.csv')\n",
    "print(dataset)\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47099627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len of each file\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102b24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6c4801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197379, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce80a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171723"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['Question'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2052c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['Context'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e88f920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question        dạ cho em hỏi em bị ngứa cổ họng ho và sổ mũi ...\n",
       "Context         Có thể là bạn bị viêm mũi họng dị ứng có kèm b...\n",
       "Answer                                                       None\n",
       "Answer_Start                                                   -1\n",
       "Answer_End                                                     -1\n",
       "Name: 243, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question = dataset.iloc[243]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb5be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using textcolor to visualize the answer within the context\n",
    "from termcolor import colored\n",
    "def color_answer(example):\n",
    "  answer_start, answer_end = example[\"Answer_Start\"],example[\"Answer_End\"]\n",
    "  context = example['Context']\n",
    "\n",
    "  return  colored(context[:answer_start], \"white\") + \\\n",
    "    colored(context[answer_start:answer_end + 1], \"green\") + \\\n",
    "    colored(context[answer_end+1:], \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49d3e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question        Dạ bác sĩ cho em hỏi với ạ. Ông , bà của em đề...\n",
      "Context         Chào bạn, ông bà bạn cao tuổi, bà có bệnh nền ...\n",
      "Answer          Chào bạn, ông bà bạn cao tuổi, bà có bệnh nền ...\n",
      "Answer_Start                                                    0\n",
      "Answer_End                                                    247\n",
      "Name: 1000, dtype: object\n",
      "\n",
      "Answer: \n",
      "\u001b[97m\u001b[0m\u001b[32mChào bạn, ông bà bạn cao tuổi, bà có bệnh nền , cả 2 ông bà đều nên tiêm nhé. Sau tiêm\n",
      "có tác dụng phụ tuỳ ông bà sốt từ 38,5 độ C uống thuốc hạ sốt, uống nhiều nước, uống thêm nước cam,\n",
      "chanh, dừa để tăng sức đề kháng và hạn chế mất nước khi sốt. \u001b[0m\u001b[97mĂn uống nhẹ nhàng đồ dễ tiêu\n",
      "thôi nhé. Dạ , dạ e cám ơn chị bác sĩ rất nhiều ạ\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[1000])\n",
    "print()\n",
    "print(\"Answer: \")\n",
    "for wrap in textwrap.wrap(color_answer(dataset.iloc[1000]), width = 100):\n",
    "  print(wrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb0345",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cab5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the base T5 model having 222M params\n",
    "MODEL_NAME ='t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa65dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458f513",
   "metadata": {},
   "source": [
    "To create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataset(Dataset):\n",
    "  def __init__(\n",
    "      self,\n",
    "      data:pd.DataFrame,\n",
    "      tokenizer:T5Tokenizer,\n",
    "      source_max_token_len: int = 396,\n",
    "      target_max_token_len: int = 32,\n",
    "\n",
    "      ):\n",
    "    \n",
    "    self.data =  data\n",
    "    self.tokenizer =  tokenizer\n",
    "    self.source_max_token_len =  source_max_token_len\n",
    "    self.target_max_token_len =  target_max_token_len\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    source_encoding = tokenizer(\n",
    "      data_row['question'],\n",
    "      data_row['context'],\n",
    "      max_length=self.source_max_token_len,\n",
    "      padding='max_length',\n",
    "      truncation=\"only_second\",\n",
    "      return_attention_mask=True,\n",
    "      add_special_tokens=True,\n",
    "      return_tensors=\"pt\"\n",
    "      )\n",
    "    \n",
    "    target_encoding = tokenizer(\n",
    "      data_row['answer_text'],\n",
    "      max_length=self.target_max_token_len,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      add_special_tokens=True,\n",
    "      return_tensors=\"pt\"\n",
    "      )\n",
    "    \n",
    "    labels = target_encoding['input_ids']\n",
    "    labels[labels==0] = -100\n",
    "\n",
    "    return dict(\n",
    "        question=data_row['question'],\n",
    "        context=data_row['context'],\n",
    "        answer_text=data_row['answer_text'],\n",
    "        input_ids=source_encoding[\"input_ids\"].flatten(),\n",
    "        attention_mask=source_encoding['attention_mask'].flatten(),\n",
    "        labels=labels.flatten()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = BioQADataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab831e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in sample_dataset:\n",
    "  print(\"Question: \", data['question'])\n",
    "  print(\"Answer text: \", data['answer_text'])\n",
    "  print(\"Input_ids: \", data['input_ids'][:10])\n",
    "  print(\"Labels: \", data['labels'][:10])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a6efa",
   "metadata": {},
   "source": [
    "Splitting into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93765d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape,  val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64592cf5",
   "metadata": {},
   "source": [
    "Create pytorch lightning datamodule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataModule(pl.LightningDataModule):\n",
    "  def __init__(\n",
    "      self,\n",
    "      train_df: pd.DataFrame,\n",
    "      test_df: pd.DataFrame,\n",
    "      tokenizer:T5Tokenizer,\n",
    "      batch_size: int = 8,\n",
    "      source_max_token_len: int = 396,\n",
    "      target_max_token_len: int = 32,\n",
    "      ):\n",
    "    super().__init__()\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.batch_size = batch_size\n",
    "    self.source_max_token_len = source_max_token_len\n",
    "    self.target_max_token_len = target_max_token_len\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    self.train_dataset = BioQADataset(\n",
    "        self.train_df,\n",
    "        self.tokenizer,\n",
    "        self.source_max_token_len,\n",
    "        self.target_max_token_len\n",
    "        )\n",
    "\n",
    "    self.test_dataset = BioQADataset(\n",
    "    self.test_df,\n",
    "    self.tokenizer,\n",
    "    self.source_max_token_len,\n",
    "    self.target_max_token_len\n",
    "    )\n",
    " \n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "        self.train_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "        )\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        num_workers=0\n",
    "        )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(\n",
    "        self.test_dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 1\n",
    "\n",
    "data_module = BioDataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b1de9",
   "metadata": {},
   "source": [
    "Loading and finetuning the T5-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fca4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87adf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the translation from English to German built-in task \n",
    "\n",
    "input_ids_translated = tokenizer(\n",
    "    \"translate English to German : Oppertunity did not knock until I built a door\",\n",
    "    return_tensors = 'pt'\n",
    ").input_ids.to(device)\n",
    "\n",
    "generated_ids = model.generate(input_ids = input_ids_translated)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8452c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_translated = [\n",
    "         tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "         for gen_id in generated_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(pred_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64068392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the summarization built-in task\n",
    "\n",
    "text = \"\"\"summarize : Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. \n",
    "He briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. \n",
    "He transferred to the University of Pennsylvania two years later, where he received bachelor's degrees in economics and physics. \n",
    "He moved to California in 1995 to attend Stanford University but decided instead to pursue a business career, \n",
    "co-founding the web software company Zip2 with his brother Kimbal. The startup was acquired by Compaq for $307 million in 1999. \n",
    "Musk co-founded online bank X.com that same year, which merged with Confinity in 2000 to form PayPal. \n",
    "The company was bought by eBay in 2002 for $1.5 billion. In 2002, Musk founded SpaceX, an aerospace manufacturer and space transport \n",
    "services company, of which he is CEO and CTO. In 2004, he joined electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.) \n",
    "as chairman and product architect, becoming its CEO in 2008. In 2006, he helped create SolarCity, a solar energy services company that \n",
    "was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit research company that promotes friendly \n",
    "artificial intelligence. In 2016, he co-founded Neuralink, a neurotechnology company focused on developing brain–computer interfaces, \n",
    "and founded The Boring Company, a tunnel construction company. Musk has proposed the Hyperloop, a high-speed vactrain transportation system.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c14990",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_summary = tokenizer(\n",
    "    text,\n",
    "    return_tensors = 'pt'\n",
    ").input_ids.to(device)\n",
    "\n",
    "generated_ids_summary = model.generate(input_ids = input_ids_summary)\n",
    "generated_ids_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summary = [\n",
    "         tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "         for gen_id in generated_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(pred_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46363db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    input_ids = encoding['input_ids'],\n",
    "    attention_mask = encoding['attention_mask'],\n",
    "    labels = labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f634b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ce917",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df4a36",
   "metadata": {},
   "source": [
    "Building the PyTorch lightning module using T5ForConditionalGeneration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQAModel(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.model(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels)\n",
    "\n",
    "    return output.loss, output.logits\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask=batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask=batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask=batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=0.0001)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a608089",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BioQAModel() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d56c8",
   "metadata": {},
   "source": [
    "Using trainer from pytorch lightning to finetune model using our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e584f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To record the best performing model using checkpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ProgressBar\n",
    "\n",
    "# Khởi tạo ProgressBar callback\n",
    "progress_bar_callback = ProgressBar()\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator='gpu',\n",
    "    devices = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7febde",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c5f19",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = BioQAModel.load_from_checkpoint(\"checkpoints/best-checkpoint.ckpt\")\n",
    "trained_model.freeze() # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0df7f",
   "metadata": {},
   "source": [
    "Generate answers for the questions in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48261fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "  source_encoding=tokenizer(\n",
    "      question[\"question\"],\n",
    "      question['context'],\n",
    "      max_length = 396,\n",
    "      padding=\"max_length\",\n",
    "      truncation=\"only_second\",\n",
    "      return_attention_mask=True,\n",
    "      add_special_tokens=True,\n",
    "      return_tensors=\"pt\"\n",
    "\n",
    "  ).to(device)\n",
    "\n",
    "  generated_ids = trained_model.model.generate(\n",
    "      input_ids=source_encoding[\"input_ids\"],\n",
    "      attention_mask=source_encoding[\"attention_mask\"],\n",
    "      num_beams=1,  # greedy search\n",
    "      max_length=80,\n",
    "      repetition_penalty=2.5,\n",
    "      early_stopping=True,\n",
    "      use_cache=True)\n",
    "  \n",
    "  preds = [\n",
    "          tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "          for generated_id in generated_ids\n",
    "  ]\n",
    "\n",
    "  return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = val_df.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f674d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12bc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question[\"answer_text\"]  # Label Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c573f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(sample_question)  # Predicted answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
