{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44986dbb",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/marib00/llamaindex-embedding-lora/blob/main/finetune_embedding_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5ac7e-d36d-4879-959a-1af414fe4c02",
   "metadata": {},
   "source": [
    "# LoRA finetuning of any Black-Box Embedding Model\n",
    "\n",
    "This notebook is based on https://github.com/run-llama/llama_index/blob/3e5d0a146fcda01a984818d381f31a19287aead8/docs/examples/finetuning/embeddings/finetune_embedding_adapter.ipynb and demonstrates how to:\n",
    "\n",
    "- Generate a fine-tuning corpus using a local LLM\n",
    "- Fine-tune a local embedding model using LoRA\n",
    "\n",
    "The latter is achieved by subclassing the `EmbeddingAdapterFinetuneEngine` and a few tricks in order to make it behave (in the way we want it to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6c5cc-8b31-41cd-95aa-6d60fbefff9b",
   "metadata": {},
   "source": [
    "## Generate Corpus\n",
    "\n",
    "We use our helper abstractions, `generate_qa_embedding_pairs`, to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b36f73f-83b1-4715-bd4d-7ce1353d1a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Tuple\u001b[38;5;66;03m#, Union\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedding\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Any, List, Optional, Tuple#, Union\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface.base import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.huggingface.pooling import Pooling\n",
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n",
    "from llama_index.finetuning.embeddings.adapter_utils import BaseAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e38d0-39ff-44e2-ab7f-fded56dcd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(files, verbose=False):\n",
    "    if verbose: print(f\"Loading files {files}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    if verbose: print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "    if verbose: print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1257dce-0be1-42c4-9346-a1fe68505fdd",
   "metadata": {},
   "source": [
    "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6d8af-5382-48b8-8a7d-98a03d2f150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = load_corpus(TRAIN_FILES, verbose=True)\n",
    "val_nodes = load_corpus(VAL_FILES, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a5f1-6fdf-473b-80ea-5ea3df5681a7",
   "metadata": {},
   "source": [
    "### Generate synthetic queries\n",
    "\n",
    "Now, we use an LLM (Mixtral) to generate questions using each text chunk in the corpus as context.\n",
    "\n",
    "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c892e-e27d-49f6-96d4-b99af330aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eddecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "model_id = 'TheBloke/Mixtral-8x7B-v0.1-GPTQ'\n",
    "code_revision = 'gptq-4bit-32g-actorder_True'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, attn_implementation='flash_attention_2')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, code_revision=code_revision, device_map='auto')\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query_wrapper_prompt=PromptTemplate('[INST] {query_str} [/INST]'),\n",
    "    context_window=16*1024,\n",
    "    max_new_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330fb1f-cfb4-4b9b-b614-06910d5330b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_qa_embedding_pairs(train_nodes, llm=llm)\n",
    "train_dataset.save_json(\"train_dataset.json\")\n",
    "\n",
    "val_dataset = generate_qa_embedding_pairs(val_nodes, llm=llm)\n",
    "val_dataset.save_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ca757-bf02-4304-a59e-7d61a12a67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release cuda memory - at this point it's probably a good idea to restart the kernel and load the data\n",
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619e9a6-4795-4ff5-bb48-ae2c50324eb2",
   "metadata": {},
   "source": [
    "## Run Embedding Finetuning\n",
    "\n",
    "Here we first define the subclasses needed for LoRA finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalAdapter(torch.nn.Identity, BaseAdapter):\n",
    "    \"\"\"Adapter model that does nothing, but includes trainable parameters \n",
    "    (e.g. LoRAs) of the embedding model, which the FinetuneEngine actually trains.\"\"\"\n",
    "    def __init__(self, embed_model):\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "    def save(self, output_path):\n",
    "        self.embed_model.save_pretrained(output_path, save_adapter=True, save_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5234bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalEmbeddingFinetuneEngine(EmbeddingAdapterFinetuneEngine):\n",
    "    \"\"\"Fintune any parameters of embed_model with requires_grad set to True, e.g. LoRA adapaters.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: EmbeddingQAFinetuneDataset,\n",
    "        embed_model: BaseEmbedding,\n",
    "        batch_size: int = 10,\n",
    "        epochs: int = 1,\n",
    "        dim: Optional[int] = None,\n",
    "        device: Optional[str] = None,\n",
    "        model_output_path: str = \"model_output\",\n",
    "        model_checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_save_steps: int = 100,\n",
    "        verbose: bool = False,\n",
    "        bias: bool = False,\n",
    "        **train_kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            embed_model=embed_model,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            adapter_model=UniversalAdapter(embed_model._model),\n",
    "            dim=dim,\n",
    "            device=device,\n",
    "            model_output_path=model_output_path,\n",
    "            model_checkpoint_path=model_checkpoint_path,\n",
    "            checkpoint_save_steps=checkpoint_save_steps,\n",
    "            verbose=verbose,\n",
    "            bias=bias,\n",
    "            **train_kwargs,\n",
    "        )\n",
    "\n",
    "    def smart_batching_collate(self, batch: List) -> Tuple[Any, Any]:\n",
    "        \"\"\"Smart batching collate.\"\"\"\n",
    "        import torch\n",
    "        from torch import Tensor\n",
    "\n",
    "        query_embeddings: List[Tensor] = []\n",
    "        text_embeddings: List[Tensor] = []\n",
    "\n",
    "        for query, text in batch:\n",
    "            query_embedding = self.embed_model.get_query_embedding(query)\n",
    "            text_embedding = self.embed_model.get_text_embedding(text)\n",
    "\n",
    "            query_embeddings.append(query_embedding)    # was stripping gradients: query_embeddings.append(torch.tensor(query_embedding))\n",
    "            text_embeddings.append(text_embedding)      # was stripping gradients: text_embeddings.append(torch.tensor(text_embedding))\n",
    "\n",
    "        query_embeddings_t = torch.stack(query_embeddings)\n",
    "        text_embeddings_t = torch.stack(text_embeddings)\n",
    "\n",
    "        return query_embeddings_t, text_embeddings_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9837a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceEmbeddingWithGrad(HuggingFaceEmbedding):\n",
    "    \"\"\"HuggingFaceEmbedding with gradient support.\"\"\"\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self._model, name)\n",
    "    \n",
    "    def _embed(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Embed sentences.\"\"\"\n",
    "        encoded_input = self._tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # pop token_type_ids\n",
    "        encoded_input.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # move tokenizer inputs to device\n",
    "        encoded_input = {\n",
    "            key: val.to(self._device) for key, val in encoded_input.items()\n",
    "        }\n",
    "\n",
    "        model_output = self._model(**encoded_input)\n",
    "\n",
    "        context_layer: \"torch.Tensor\" = model_output[0]\n",
    "        if self.pooling == Pooling.CLS:\n",
    "            embeddings = self.pooling.cls_pooling(context_layer)\n",
    "        elif self.pooling == Pooling.LAST:\n",
    "            embeddings = self.pooling.last_pooling(context_layer)           \n",
    "        else:\n",
    "            embeddings = self._mean_pooling(\n",
    "                token_embeddings=context_layer,\n",
    "                attention_mask=encoded_input[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "        if self.normalize:\n",
    "            import torch\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings  # was embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import fields as pydantic_fields\n",
    "\n",
    "class disable_pydantic:\n",
    "    \"\"\"Context manager to disable pydantic validation.\"\"\"\n",
    "\n",
    "    def __enter__(self) -> None:\n",
    "        self.validate = pydantic_fields.ModelField.validate\n",
    "        pydantic_fields.ModelField.validate = lambda *args, **kwargs: (args[1], None)\n",
    "\n",
    "    def __exit__(self, *args) -> None:\n",
    "        pydantic_fields.ModelField.validate = self.validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb224e",
   "metadata": {},
   "source": [
    "### Fine-tune sfr-embedding-mistral\n",
    "\n",
    "As of March 2024 SFR-Embedding-Mistral is at the top of the Massive Text Embedding Benchmark (MTEB) Leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef7b91",
   "metadata": {},
   "source": [
    "We quantize the model to 4-bit first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Salesforce/SFR-Embedding-Mistral'\n",
    "quant_path = f'/tmp/models/{model_id.replace(\"/\",\"-\")}-quant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# freeze the model before saving just as a precaution\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.save_pretrained(quant_path, low_cpu_mem_usage=False)\n",
    "print(f'Quantized model saved to {quant_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09683b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release cuda memory\n",
    "del model, tokenizer, bnb_config\n",
    "import gc; gc.collect()\n",
    "with torch.no_grad(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_adapters_path = '/tmp/whatever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bed4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "embed_model = AutoModel.from_pretrained(quant_path, low_cpu_mem_usage=True)\n",
    "embed_model.to = lambda _: embed_model  # quantized model does not have .to() method\n",
    "for param in embed_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_base_model = HuggingFaceEmbedding(\n",
    "    model=embed_model, \n",
    "    tokenizer=embed_tokenizer, \n",
    "    query_instruction='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:',\n",
    "    pooling='last',\n",
    "    embed_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf592ed6",
   "metadata": {},
   "source": [
    "Evaluate the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3068379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import evaluate, display_results\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_sfr_val_results = evaluate(val_dataset, hf_base_model)\n",
    "display_results([\"base_sfr\"], [base_sfr_val_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605768ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the peft model\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    ")\n",
    "\n",
    "kbit_model = prepare_model_for_kbit_training(embed_model)\n",
    "peft_model = get_peft_model(kbit_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or  load trained adapters\n",
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(embed_model, lora_adapters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_qlora_model = HuggingFaceEmbeddingWithGrad(\n",
    "    model=peft_model, \n",
    "    tokenizer=embed_tokenizer, \n",
    "    query_instruction='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:',\n",
    "    pooling='last',\n",
    "    embed_batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine = UniversalEmbeddingFinetuneEngine(\n",
    "    train_dataset,\n",
    "    embed_model=hf_qlora_model,\n",
    "    dim=4096,\n",
    "    model_output_path=lora_adapters_path,\n",
    "    epochs=5,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "with disable_pydantic():\n",
    "    finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repackage as HuggingFaceEmbedding to avoid grief from pydantic which wants embeddings to be lists not tensors\n",
    "hf_embeddig_model = HuggingFaceEmbedding(\n",
    "    model=hf_qlora_model.model, \n",
    "    tokenizer=hf_qlora_model._tokenizer, \n",
    "    query_instruction=hf_qlora_model.query_instruction,\n",
    "    pooling=hf_qlora_model.pooling,\n",
    "    embed_batch_size=hf_qlora_model.embed_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25a60e",
   "metadata": {},
   "source": [
    "Evaluate the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import evaluate, display_results\n",
    "\n",
    "with torch.no_grad():\n",
    "    lora_sfr_val_results = evaluate(val_dataset, hf_embeddig_model)\n",
    "display_results([\"lora_sfr\"], [lora_sfr_val_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5e9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
